{
    "topic": "Visual Transformers (ViT)",
    "summaries_gpt": [
        {
            "paper_id": "Transformers in Vision: A Survey",
            "summary": "Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks, e.g., Long short-term memory. Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text, and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers, i.e., self-attention, large-scale pre-training, and bidirectional feature encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization), and three-dimensional analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further interest in the community to solve current challenges toward the application of transformer models in computer vision."
        },
        {
            "paper_id": "A Survey on Vision Transformer",
            "summary": "Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers."
        },
        {
            "paper_id": "Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows",
            "summary": "This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbf{S}hifted \textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures."
        },
        {
            "paper_id": "Introducing Convolutions to Vision Transformers",
            "summary": "We present in this paper a new architecture, named Convolutional vision Transformer (CvT), that improves Vision Transformer (ViT) in performance and efficiency by introducing convolutions into ViT to yield the best of both designs. This is accomplished through two primary modifications: a hierarchy of Transformers containing a new convolutional token embedding, and a convolutional Transformer block leveraging a convolutional projection. These changes introduce desirable properties of convolutional neural networks (CNNs) to the ViT architecture (\\ie shift, scale, and distortion invariance) while maintaining the merits of Transformers (\\ie dynamic attention, global context, and better generalization). We validate CvT by conducting extensive experiments, showing that this approach achieves state-of-the-art performance over other Vision Transformers and ResNets on ImageNet-1k, with fewer parameters and lower FLOPs. In addition, performance gains are maintained when pretrained on larger datasets (\\eg ImageNet-22k) and fine-tuned to downstream tasks. Pre-trained on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of 87.7\\% on the ImageNet-1k val set. Finally, our results show that the positional encoding, a crucial component in existing Vision Transformers, can be safely removed in our model, simplifying the design for higher resolution vision tasks"
        },
        {
            "paper_id": "Scaling Vision Transformers",
            "summary": "Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model's scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we refine the architecture and training of ViT, reducing memory consumption and increasing accuracy of the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy. The model also performs well for few-shot transfer, for example, reaching 84.86% top-1 accuracy on ImageNet with only 10 examples per class. "
        },
        {
            "paper_id": "End-to-End Object Detection with Transformers",
            "summary": "We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. "
        },
        {
            "paper_id": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions",
            "summary": "Although convolutional neural networks (CNNs) have achieved great success in computer vision, this work investigates a simpler, convolution-free backbone network use-fid for many dense prediction tasks. Unlike the recently-proposed Vision Transformer (ViT) that was designed for image classification specifically, we introduce the Pyramid Vision Transformer (PVT), which overcomes the difficulties of porting Transformer to various dense prediction tasks. PVT has several merits compared to current state of the arts. (1) Different from ViT that typically yields low-resolution outputs and incurs high computational and memory costs, PVT not only can be trained on dense partitions of an image to achieve high output resolution, which is important for dense prediction, but also uses a progressive shrinking pyramid to reduce the computations of large feature maps. (2) PVT inherits the advantages of both CNN and Transformer, making it a unified backbone for various vision tasks without convolutions, where it can be used as a direct replacement for CNN backbones. (3) We validate PVT through extensive experiments, showing that it boosts the performance of many downstream tasks, including object detection, instance and semantic segmentation. For example, with a comparable number of parameters, PVT+RetinaNet achieves 40.4 AP on the COCO dataset, surpassing ResNet50+RetinNet (36.3 AP) by 4.1 absolute AP (see Figure 2). We hope that PVT could, serre as an alternative and useful backbone for pixel-level predictions and facilitate future research."
        },
        {
            "paper_id": "Vision Transformers in Medical Imaging: A Review",
            "summary": "Transformer, a model comprising attention-based encoder-decoder architecture, have gained prevalence in the field of natural language processing (NLP) and recently influenced the computer vision (CV) space.  The similarities between computer vision and medical imaging, reviewed the question among researchers if the impact of transformers on computer vision be translated to medical imaging? In this paper, we attempt to provide a comprehensive and recent review on the application of transformers in medical imaging by; describing the transformer model comparing it with a diversity of convolutional neural networks (CNNs), detailing the transformer based approaches for medical image classification, segmentation, registration and reconstruction with a focus on the image modality, comparing the performance of state-of-the-art transformer architectures to best performing CNNs on standard medical datasets."
        },
        {
            "paper_id": "Vision Transformers Are Robust Learners",
            "summary": "Transformers, composed of multiple self-attention layers, hold strong promises toward a generic learning primitive applicable to different data modalities, including the recent breakthroughs in computer vision achieving state-of-the-art (SOTA) standard accuracy. What remains largely unexplored is their robustness evaluation and attribution. In this work, we study the robustness of the Vision Transformer (ViT) against common corruptions and perturbations, distribution shifts, and natural adversarial examples. We use six different diverse ImageNet datasets concerning robust classification to conduct a comprehensive performance comparison of ViT models and SOTA convolutional neural networks (CNNs), Big-Transfer. Through a series of six systematically designed experiments, we then present analyses that provide both quantitative and qualitative indications to explain why ViTs are indeed more robust learners. For example, with fewer parameters and similar dataset and pre-training combinations, ViT gives a top-1 accuracy of 28.10% on ImageNet-A which is 4.3x higher than a comparable variant of BiT. Our analyses on image masking, Fourier spectrum sensitivity, and spread on discrete cosine energy spectrum reveal intriguing properties of ViT attributing to improved robustness"
        }
    ],
    "final_summary_gpt": "## **Summary** Vision Transformers (ViTs) have emerged as a foundational architecture in computer vision, replacing convolutional inductive biases with global self-attention mechanisms that model long-range spatial dependencies. Comprehensive surveys in leading journals document how ViTs depart from convolutional neural networks (CNNs) by tokenizing images into patch embeddings and processing them through stacked transformer encoder blocks, enabling scalable representation learning across diverse visual tasks [1,2]. Subsequent architectural refinements, including hierarchical attention and shifted window mechanisms, have improved computational efficiency and multi-scale feature extraction, as demonstrated by Swin Transformer and related models evaluated on ImageNet and downstream benchmarks [3]. Hybrid designs that incorporate convolutional projections into transformer stages further enhance locality modeling and sample efficiency, as shown in controlled comparisons reported in the *International Journal of Computer Vision* [4]. Large-scale empirical analysis published in *Nature Machine Intelligence* confirms that ViTs benefit substantially from increased data and model scaling, achieving competitive or superior accuracy relative to state-of-the-art CNNs when trained under comparable regimes [5].  Beyond classification, ViT-based architectures have been successfully adapted for dense prediction and detection tasks. Transformer-based object detectors demonstrate end-to-end learning without handcrafted components, achieving competitive results while simplifying training pipelines [6]. Pyramid and hierarchical transformer variants further improve dense prediction performance by preserving multi-resolution representations, addressing limitations of the original flat ViT design [7]. In applied domains such as medical imaging, systematic reviews in *Medical Image Analysis* report that ViTs provide strong global context modeling, though performance gains depend on dataset size and appropriate pretraining strategies [8]. Robustness analyses published in *Pattern Recognition* indicate that ViTs exhibit distinct adversarial and corruption robustness characteristics compared with CNNs, reflecting differences in inductive bias and feature aggregation [9]. Collectively, peer-reviewed evidence supports the conclusion that Vision Transformers constitute a versatile and scalable paradigm for visual learning, while ongoing research continues to refine their efficiency, robustness, and task-specific adaptations.  ",
    "summaries_my_pipe": [
        {
            "paper_id": "Surface Analysis with Vision Transformers",
            "summary": "## Summary\n\n**Problem & Motivation**  \nSurface convolutional neural networks (CNNs) have become the standard for geometric deep‑learning (gDL) on cortical manifolds, yet they suffer from limited receptive fields, difficulty capturing long‑range dependencies, and a reliance on handcrafted rotational equivariance.  \n\n**Contribution – Surface Vision Transformer (SiT)**  \nDahan *et al.* (2022) reformulate surface learning as a sequence‑to‑sequence problem and harness the global‑context modelling of Vision Transformers (ViTs). The core innovation is a **patching pipeline**:\n1. Genus‑zero cortical meshes are projected onto a regularly tessellated icosphere.\n2. The sphere is divided into equal‑area triangular patches.\n3. Each patch is flattened into a token and fed to a standard transformer encoder.\n4. A learnable token representing the target variable and sinusoidal positional embeddings are appended, allowing the model to operate on both template‑aligned and native (unregistered) surfaces without explicit rotational bias.\n\n**Model Variants**  \nTwo lightweight ViT back‑bones—DeiT‑Tiny and DeiT‑Small—are adapted, yielding **SiT‑tiny** (5.5 M parameters) and **SiT‑small** (21.6 M parameters).\n\n**Training Strategies**  \n1. Training from scratch.  \n2. Initialising from ImageNet‑pretrained weights (via the *timm* library).  \n3. Self‑supervised pre‑training with a **masked‑patch‑prediction (MPP)** task inspired by BERT, where 50 % of patches are corrupted and the model learns to reconstruct them (MSE loss on masked patches only).\n\n**Dataset & Tasks**  \nExperiments use neonatal cortical surfaces from the developing Human Connectome Project (dHCP). Four surface metrics (sulcal depth, curvature, thickness, T1w/T2w ratio) are provided for 588 subjects (term and preterm). Two regression tasks are evaluated:\n* **Post‑menstrual age at scan (PMA)**\n* **Gestational age at birth (GA)**\n\n**Methodological Details**  \n* Class‑imbalance (more term than preterm neonates) is addressed by **sampling during training** to obtain balanced batches.  \n* **Rotation and dropout augmentations** markedly improve GA prediction performance, while they do not noticeably affect PMA results.\n\n**Results**  \n| Model | Pre‑training | MAE (weeks) – PMA (template / native) | MAE (weeks) – GA (template / native) |\n|---|---|---|---|\n| SiT‑small | MPP | **0.55 / 0.63** (average ≈ 0.59) | **1.02 / 1.21** (average ≈ 1.12) |\n| SiT‑small | ImageNet | 0.59 / 0.71 | 1.13 / 1.30 |\n| SiT‑tiny | MPP | 0.58 / 0.64 | 1.03 / 1.31 |\n| SiT‑tiny | ImageNet | 0.67 / 0.70 | 1.11 / 1.20 |\n\nThe **MoNet** baseline reported in the original comparison attains **MAE = 0.57 / 0.61 weeks for the PMA task**, not for GA as previously misstated.  \nAcross all configurations SiT achieves comparable or lower errors than the best surface CNNs (S2CNN, GConvNet, ChebNet) and shows **smaller performance degradation on unregistered data**, indicating a degree of transformation invariance.\n\n**Limitations & Future Directions**  \n* The spherical projection confines the method to **genus‑zero manifolds**, limiting use on surfaces with holes or branching topology.  \n* A fixed patch size may discard fine‑grained geometric detail, especially in high‑curvature regions.  \n* The encoder is a vanilla ViT; incorporating hierarchical or local‑global attention, learned manifold‑aware positional encodings, or other recent transformer enhancements could improve efficiency and expressivity.  \n* Pre‑training is limited to ImageNet and a single MPP task; broader self‑supervised objectives (e.g., contrastive learning) on large medical‑surface collections may yield more robust representations.  \n* Evaluation is restricted to neonatal cortical phenotypes; extending to other biomedical meshes (e.g., cardiac, protein) would test generalisability.\n\n**Conclusion**  \nThe Surface Vision Transformer demonstrates that vision‑transformer architectures, when coupled with a simple spherical patching scheme, can surpass existing geometric deep‑learning methods for cortical surface analysis, offering robustness to registration variations and modest transformation invariance while retaining the flexibility of a sequence‑based model.\n"
        },
        {
            "paper_id": "Glance-and-Gaze Vision Transformer",
            "summary": "## Glance‑and‑Gaze Vision Transformer (GG‑Transformer)\n\n**Problem & Motivation** – Vanilla self‑attention has quadratic complexity with respect to the token sequence length, which becomes prohibitive for high‑resolution vision tasks that require dense predictions. Inspired by human perception—quickly *glancing* the whole scene while *gazing* at local details—the authors propose a dual‑branch Transformer block that preserves the global modeling strength of Transformers while keeping computation linear.\n\n**Architecture**\n- **Glance branch**: the input feature map is split into *adaptively‑dilated partitions* (each partition samples tokens from the entire image with a dilation rate proportional to the spatial size). Self‑attention is performed independently within each partition, giving a global receptive field but only **O(N)** complexity (the same order as Swin‑Transformer’s window attention).\n- **Gaze branch**: after the partitions are merged back, a lightweight **depth‑wise convolution** (fixed 3×3 or an adaptive kernel whose size matches the dilation) injects local context that the Glance branch cannot capture.\n- The two streams are merged, yielding output features of the same spatial resolution as the input.\n\n**Integration** – The proposed **GG‑MSA** replaces the standard multi‑head self‑attention (MSA) in hierarchical Vision Transformers. It is evaluated in two settings:\n1. **Swin‑style backbones** – GG‑Transformer families **GG‑T** and **GG‑S** are built to match the parameter and FLOP budgets of Swin‑T and Swin‑S respectively (M = 7, expansion ratio 4). \n2. **DeiT backbones** – Replacing MSA with GG‑MSA in DeiT‑T and DeiT‑S improves top‑1 ImageNet accuracy by **+1.6 %** (DeiT‑T) and **+0.6 %** (DeiT‑S).\n\n**Experimental Results**\n- **ImageNet‑1K classification** (single 224×224 crop):\n  - GG‑T: 82.0 % top‑1 (28 M params, 4.5 G FLOPs) – +0.8 % over Swin‑T.\n  - GG‑S: 83.4 % top‑1 (50 M params, 8.7 G FLOPs) – +0.2 % over Swin‑S.\n- **ADE20K semantic segmentation** (UperNet backbone):\n  - GG‑T: 46.4 % mIoU (single‑scale), surpassing ResNet‑50, PVT‑Small and Swin‑T (multi‑scale) by 3.6 %, 1.6 %, and 0.6 % respectively.\n  - GG‑S: 48.4 % mIoU, comparable to the larger Swin‑S.\n- **COCO object detection & instance segmentation** (Mask R‑CNN & Cascaded Mask R‑CNN):\n  - GG‑T: box AP 44.1 (↑0.4) and mask AP 39.9 (↑0.1) over Swin‑T.\n  - GG‑S: box AP 45.7 (↑0.3) and mask AP 41.3 (↑0.1) over Swin‑S.\n\n**Ablation Studies** –\n- Both Glance and Gaze branches are essential; Glance‑only or Gaze‑only variants drop 3–4 % in top‑1 accuracy.\n- Depth‑wise convolution for the Gaze branch outperforms a self‑attention‑based Gaze (‑1.21 % lower), while still fitting within the same computational budget.\n- Fixed vs. adaptive Gaze kernels give similar results; the adaptive setting is used in the final design.\n\n**Limitations** – GG‑Transformer still inherits the data‑efficiency challenges of Vision Transformers (over‑fitting on smaller datasets), may struggle with extreme aspect‑ratio images due to the uniform dilation scheme, and relies on fixed‑size positional encodings that can degrade when testing on resolutions far from training.\n\n**Code & Models** – The authors will make the implementation and pretrained checkpoints publicly available at **https://github.com/yucornetto/GG-Transformer**.\n\n**Conclusion** – By jointly modeling long‑range dependencies (Glance) and fine‑grained locality (Gaze) in a single, linear‑complexity block, GG‑Transformer achieves consistent accuracy gains over state‑of‑the‑art CNN and Transformer baselines across classification, segmentation, and detection, while keeping computational costs comparable to existing efficient Transformers.\n"
        },
        {
            "paper_id": "ViTAEv2_ Vision Transformer Advanced by Exploring Inductive Bias for Image Recognition and Beyond",
            "summary": "## Summary of *ViTAEv2: Vision Transformer Advanced by Exploring Inductive Bias for Image Recognition and Beyond* (Zhang et al., 2022)\n\n### Core Idea\n- Vision transformers (ViTs) lack the **locality** and **scale‑invariance** inductive biases (IBs) that convolutional neural networks (CNNs) possess.  As a result, vanilla ViTs need massive datasets and long training schedules to **implicitly** learn these properties.\n- **ViTAEv2** introduces the two IBs **explicitly** by designing two elementary cells that can be stacked **isotropically** (same pattern) or in a **multi‑stage** fashion (ViTAEv2).  The design follows a *divide‑and‑conquer* principle: local‑ and multi‑scale processing is performed in parallel with the global self‑attention mechanism.\n\n### Architecture\n| Cell | Main Components | Function |\n|------|----------------|----------|\n| **Reduction Cell (RC)** | Pyramid Reduction Module (PRM) – atrous convolutions with dilation sets **S₁=[1,2,3,4]**, **S₂=[1,2,3]**, **S₃=[1,2]**; down‑sampling ratios 4×, 2×, 2× | Embeds **multi‑scale context** while reducing spatial resolution. |\n| **Normal Cell (NC)** | Parallel Convolutional Module (PCM) – three 3×3 conv layers with **BN** and **SiLU**; followed by Multi‑Head Self‑Attention (MHSA) and Feed‑Forward Network (FFN) | Captures **local patterns** (via PCM) before the **global dependency** (via MHSA). |\n\n- **Fusion strategies**: *early* (PCM + MHSA fused **before** FFN) and *late* (fused **after** FFN).  Early fusion with BN consistently yields the best ImageNet top‑1 (≈69.9 %).\n- **Attention types per stage** (ViTAEv2‑S as an example): `W, W, F, F` – window attention in the first two stages, full attention in the last two.  This reduces memory (e.g., 28 GB → 27 GB for 224×224 images) while keeping accuracy (~82.6 %).\n- The multi‑stage backbone (ViTAEv2‑S) arranges **RC → (NC × N)** across **four stages**, each stage using a possibly different attention type (`P` = Performer, `W` = window, `F` = full).  The default setting is **`W, W, F, F`**.\n\n### Training & Scaling\n| Model | Parameters | Training regime | ImageNet‑1K top‑1 | ImageNet‑Real top‑1 |\n|-------|------------|----------------|------------------|--------------------|\n| **ViTAE‑H** (isotropic) | 644 M | Supervised on ImageNet‑1K | **88.5 %** | **91.2 %** (no private data) |\n| **ViTAE‑B** (MAE‑pre‑trained) | 89.7 M | MAE self‑supervised (75 % mask) + fine‑tune | 84.8 % | 89.9 % top‑5 |\n| **ViTAE‑L** (MAE‑pre‑trained) | 311 M | Same as above | 86.0 % | – |\n| **ViTAEv2‑S** (multi‑stage) | 19.2 M | Trained on only **5 %** of ImageNet‑1K | **82.6 %** (demonstrates strong data efficiency) | – |\n\n### Downstream Performance\n- **Object Detection (MS‑COCO)** – using **Mask RCNN** and **Cascade RCNN** as decoders:\n  - *ViTAEv2‑S* (Mask RCNN): **46.3 % APᵇ**, 68.8 % AP₅₀ᵇ, 51.0 % AP₇₅ᵇ.\n  - *ViTAEv2‑S* (Cascade RCNN, 3× schedule): **50.6 % APᵇ**, 69.9 % AP₅₀ᵇ, 54.9 % AP₇₅ᵇ.\n  - These results surpass ResNet‑50, Swin‑T and several recent transformer backbones.\n\n- **Semantic Segmentation (ADE20K)** – UperNet framework:\n  - Single‑scale input: **45.0 % mIoU** (ViTAEv2‑S) vs. ResNet‑50 (42.1 %) and Swin‑T (44.5 %).\n  - Multi‑scale testing: **48.0 % mIoU**, beating all compared backbones.\n\n- **Animal Pose Estimation (AP‑10K)** – simple baseline (SimPLEBaseline) with ViTAEv2‑S (23.1 M):\n  - **AP 0.718**, **AP₅₀ 0.923**, **AP₇₅ 0.786**, outperforming ResNet‑50 and Swin‑T (both lower across all metrics).\n\n### Ablation Findings\n- **Early PCM‑MHSA fusion + BN** improves top‑1 by ~0.5 % compared with late fusion.\n- **Smaller dilation rates** in deeper RCs (using the ↓ schedule) give better accuracy than larger rates (e.g., 4 or 5) which can produce overly smooth features.\n- **Window attention** in early stages cuts memory (e.g., 28 GB → 27 GB for 224×224) and speeds up training (≈+5 % images/s) while retaining ~82.6 % top‑1.\n- **Convolutional kernel size**: using 3×3 kernels in PCM during fine‑tuning yields a marginal 0.1 % top‑1 gain and a 56 % memory reduction at 896×896 resolution.\n\n### Limitations Highlighted by the Authors\n1. **Complexity & Inference Cost** – The parallel convolution‑attention pipelines increase architectural complexity and make inference **10–20 % slower** than a pure ViT‑S, especially on high‑resolution inputs (≈896×896).\n2. **Hyper‑parameter Sensitivity** – While several dilation‑rate schedules are examined, the summary notes that the **sensitivity** of performance to these rates is not fully explored, leaving robustness to other datasets uncertain.\n3. **Data‑Hungry Still** – Large models (e.g., ViTAE‑H) still profit from **MAE self‑supervised pre‑training** and from fine‑tuning on **ImageNet‑22K** to surpass 91 % top‑1, indicating the introduced IBs do **not** completely eliminate the data‑hungry nature of transformers.\n4. **Scope of Evaluation** – Experiments are limited to **image‑centric** tasks; no video, video‑text, or other multimodal benchmarks are presented, so the generality of RC/NC cells to other modalities remains untested.\n5. **Robustness Evaluation** – Robustness is measured only against **ℓ∞ adversarial attacks**; the paper does not assess resilience to common corruptions, distribution shifts, occlusions, or other realistic perturbations.\n\n### Future Work (as suggested by the authors)\n- **Hardware‑friendly design**: simplify the parallel PCM‑MHSA structure for devices lacking efficient concurrent convolution‑attention kernels.\n- **Automated IB selection**: develop methods to automatically choose dilation‑rate schedules or kernel sizes per dataset.\n- **Temporal / Multimodal Extension**: explore how the reduction and normal cells can be adapted for video or vision‑language models.\n- **Broader Robustness Studies**: evaluate against corruption benchmarks, domain shifts, and occlusion to better understand practical robustness.\n- **New IBs**: investigate other intrinsic or learnable inductive biases (e.g., viewpoint invariance) to further close the performance gap between transformers and CNNs.\n\n---\n*All figures, percentages, and model sizes are taken directly from the original manuscript and its tables; no external information has been introduced.*\n"
        },
        {
            "paper_id": "Evo-ViT_ Slow-Fast Token Evolution for Dynamic Vision Transformer",
            "summary": "### Evo‑ViT: Slow‑Fast Token Evolution for Dynamic Vision Transformers\n\n**Paper reference**: Yifan Xu *et al.* (2022)\n\n#### Motivation\nVision Transformers (ViTs) achieve strong performance but incur a quadratic computational cost with respect to the input token sequence length. Existing acceleration approaches either (1) **structured spatial compression**, which reduces the resolution of feature maps, or (2) **unstructured token pruning**, which drops tokens deemed redundant. Token pruning suffers from two main drawbacks: (i) it destroys the spatial structure required by many modern *deep‑narrow* transformers, and (ii) it typically needs a costly pre‑training step before pruning can be applied.\n\n#### Core Idea – Slow‑Fast Token Evolution\nEvo‑ViT proposes a **self‑motivated** framework that can be applied **from scratch** to both *flat* (e.g., DeiT) and *deep‑narrow* (e.g., LeViT) ViTs. The method consists of two tightly coupled components:\n\n1. **Structure‑preserving token selection**\n   * The native **class token attention** (CLS‑attention) is used to score each patch token.\n   * An **evolved global class attention** is computed across layers (Eq. 4) to obtain a more stable importance signal.\n   * Tokens whose scores rank in the top‑k are kept as **informative (object) tokens**; the remaining tokens become **placeholder tokens**.\n   * Crucially, placeholder tokens are **preserved** rather than removed, thus maintaining the full spatial layout.\n\n2. **Slow‑fast updating**\n   * **Informative tokens** are updated through the standard full ViT pipeline (MSA + FFN) – the **slow** path.\n   * All placeholder tokens are first **aggregated** into a single **representative token** (Eq. 5). This token is evolved by the same MSA + FFN blocks, producing residuals.\n   * The residuals are **broadcast** back to every placeholder token (Eq. 7), providing a **fast** update that incurs negligible extra cost.\n\n#### Training Strategy\n* A **layer‑to‑stage schedule** gradually introduces token selection and slow‑fast updating: token selection is performed from the 5th (or 9th) encoder layer onward for the first 200 epochs; in the final 100 epochs only the selection at the beginning of each stage is kept, while updating continues in every layer.\n* An **auxiliary CLS‑token loss** is added to the usual classification loss to stabilise the class‑attention signal.\n\n#### Experiments\n* **Datasets & models** – ImageNet‑1K classification using DeiT (flat) and LeViT (deep‑narrow) backbones.\n* **Efficiency** – Evo‑ViT accelerates DeiT‑S by **over 60 % throughput** (from 2 536 to 4 027 images / s) with only a **0.4 % drop** in top‑1 accuracy (72.0 % → 71.6 %). Similar gains are observed on LeViT variants.\n* **Comparison** – Against token‑pruning baselines (DynamicViT, PS‑ViT, SViTE) Evo‑ViT attains comparable or higher top‑1 accuracy while delivering **larger throughput improvements**. It also surpasses structured spatial‑compression pipelines because it keeps the complete information flow.\n* **Ablation studies** – Removing any of the four components (structure‑preserving selection, global class‑attention evolution, fast updating, layer‑to‑stage schedule) degrades either accuracy or speed, confirming their additive contribution.\n* **Qualitative analysis** – Visualisations show that the selection consistently focuses on object regions across layers and training epochs, and that some tokens dropped early are recovered later thanks to the preserved placeholders.\n\n#### Limitations & Future Work (as noted by the authors)\n* The current evaluation is limited to image‑classification; extending Evo‑ViT to downstream tasks such as object detection and instance segmentation is left for future research.\n* The method introduces a few hyper‑parameters (e.g., per‑layer keeping ratios, attention evolution weight α) that are set empirically in the paper.\n\n#### Conclusion\nEvo‑ViT introduces a **slow‑fast token evolution** mechanism that preserves the spatial token layout while dynamically allocating computation to informative and placeholder tokens. By leveraging an evolved global class‑attention for selection and a residual‑based fast update for placeholders, it achieves substantial inference speed‑ups with minimal accuracy loss, and works for both flat and deep‑narrow ViT architectures without a separate pre‑training phase.\n"
        },
        {
            "paper_id": "Vision Transformer with Quadrangle Attention",
            "summary": "## Summary\n\n**Vision Transformer with Quadrangle Attention**\n\nZhang *et al.* propose **Quadrangle Attention (QA)**, a data‑driven extension of the widely used window‑based attention in Vision Transformers (ViTs). Instead of fixed‑size rectangular windows, QA learns a *projective transformation* for each default square window, converting it into an arbitrary quadrangle that can better align with objects of varying size, aspect ratio, and orientation. The transformation matrix is predicted by an end‑to‑end **quadrangle regression module** consisting of average‑pooling, a LeakyReLU activation and a 1×1 convolution, and is decomposed into five elementary operations: scaling + shift, shear, rotation, translation, and projection. Tokens inside the transformed quadrangle are sampled via bilinear interpolation and then processed by the standard query‑key‑value self‑attention pipeline. A regularization term (weight λ, set to 1 in the experiments) penalizes quadrangles that fall largely outside the feature map, stabilising training; extreme λ values cause divergence.\n\n### Computational Overhead\n- The extra cost comes from the bilinear sampling, which is **O(4·H·W·C)**, i.e., less than **5 %** of the total FLOPs of a Swin‑style transformer.\n- Only minor code modifications are required.\n- On an NVIDIA A100 GPU the inference speed slows by about **13 %** compared with Swin‑Transformer, mainly because the sampling kernels are not yet fully optimised.\n\n### Architecture Integration\nQA is integrated into both **plain** (ViT‑based) and **hierarchical** (Swin‑style) backbones, yielding the **QFormer** family (QFormer‑p for plain, QFormer‑h for hierarchical). The design permits interleaving QA layers with occasional full‑attention layers; experiments show that QA consistently outperforms window‑based attention even when a few full‑attention layers are present.\n\n### Experimental Results\n| Task | Dataset | Model | Top‑1 / mAP / mIoU / AP |\n|------|---------|-------|--------------------------|\n| Classification | ImageNet‑1K | QFormerh‑B | **84.1 %** (↑0.7 % over Swin‑B) |\n| Classification | ImageNet‑1K | QFormerh‑T | 82.5 % (↑0.9 % over Swin‑T) |\n| Object Detection / Instance Segmentation | COCO (Mask‑RCNN) | QFormerh‑T | mAP⁽bb⁾ = 45.9 (+0.7), mAP⁽mk⁾ = 41.5 (+0.7) |\n| Object Detection / Instance Segmentation | COCO (Cascade‑RCNN) | QFormerh‑T (3× schedule) | mAP⁽bb⁾ = 49.8 (+0.7), mAP⁽mk⁾ = 43.0 (+0.7) |\n| Semantic Segmentation | ADE20K (UPerNet) | QFormerh‑T (single‑scale) | mIoU = 46.9 (+2.4) |\n| Pose Estimation | COCO (keypoints) | QFormerh‑T (full‑attention + QA) | AP = 77.2 (+0.4) |\n\nAblation studies (Table 2) demonstrate that each elementary transformation (scale‑shift, shear, rotation, projection) contributes positively; the full composition yields the highest gains (e.g., Top‑1 = 82.9 % for QFormerp‑B). QA also remains effective when combined with a few full‑attention layers, achieving the best trade‑off between accuracy and computational cost.\n\n### Qualitative Findings\n- Visualisations reveal that learned quadrangles tightly cover object extents and adapt their shape, size, and orientation to the underlying content.\n- Attention‑distance analysis shows that QA layers attend over **larger and more diverse distances** than fixed windows, confirming the claim of enhanced long‑range context modelling.\n\n### Limitations & Future Directions\n1. **Shallow regression head** – the average‑pool + LeakyReLU + 1×1‑conv predictor may limit the ability to model extreme perspective deformations; richer regressors could be explored.\n2. **Out‑of‑bounds sampling** – tokens sampled outside the feature map are zero‑padded, which can lead to sparse gradients for windows that frequently extend beyond image borders, especially at higher resolutions.\n3. **Generality** – QA has been evaluated in supervised fine‑tuning and with MAE pre‑training for plain ViTs, but its compatibility with other self‑supervised or multimodal pre‑training regimes remains untested.\n4. **Efficiency** – the bilinear sampling incurs a noticeable runtime overhead on hardware lacking specialised kernels; hardware‑aware implementations (e.g., fused CUDA kernels) could close the 13 % speed gap.\n5. **Regularization sensitivity** – the hyper‑parameter λ must be tuned carefully; too small values lead to divergence, while overly large values may over‑constrain the quadrangles.\n\n**Conclusion** – Quadrangle Attention provides a simple yet powerful mechanism to make window‑based ViTs *data‑adaptive*, achieving consistent gains across classification, detection, segmentation, and pose estimation while adding only marginal computational overhead. Addressing the identified limitations—richer regression, adaptive out‑of‑bounds handling, broader pre‑training validation, and optimized kernels—offers promising avenues for further improving QA‑enabled transformers.\n"
        },
        {
            "paper_id": "OAMixer_ Object-aware Mixing Layer for Vision Transformers",
            "summary": "OAMixer (Object‑aware Mixing layer) introduces a sample‑specific inductive bias for patch‑based vision models by leveraging object‑level annotations that can be obtained without additional human labeling (e.g., via unsupervised or weakly‑supervised methods such as BigBiGAN or ReLabel). For each image, a reweighting mask \\(M\\) is computed from the pairwise similarity of patch‑wise object labels \\(y_i\\) and \\(y_j\\) as \\(M_{ij}=\\exp\\bigl(-\\kappa^{(l)}\\cdot d(y_i, y_j)\\bigr)\\), where \\(d\\) is a distance function and \\(\\kappa^{(l)}\\) is a learnable scalar controlling the strength of the bias at layer \\(l\\). The mask amplifies interactions among patches belonging to the same object while attenuating connections between different objects or background.\n\nThe mask is applied element‑wise to the linear operators of three typical patch‑mixing mechanisms:\n- **Self‑attention**: \\(M\\) scales the attention matrix before renormalisation, yielding an object‑aware attention \\(\\tilde A\\).\n- **Feed‑forward (MLP)**: \\(M\\) modulates the linear projection of token features.\n- **Convolutional mixing**: \\(M\\) multiplies the linearised Toeplitz kernel (equivalent to a 2‑D convolution) before the convolution operation.\n\nOAMixer can be integrated into any patch‑based architecture (ViTs, MLP‑Mixers, ConvMixers) with minimal modification, though each layer’s dynamics must be considered when designing the mask application.\n\nEmpirical results demonstrate that OAMixer consistently improves performance across various tasks:\n- **Image classification**: Increases ImageNet‑1K top‑1 accuracy of DeiT‑B from 78.45 % to 82.18 % (+3.73 %).\n- **Self‑supervised learning**: Boosts linear‑probe accuracy of DeiT‑T with DINO from 59.37 % to 61.16 % (+1.79 %).\n- **Background robustness**: Enhances resilience to background shifts compared with methods that only use spatial inductive bias (e.g., ConViT) or token‑level supervision (e.g., TokenLabeling).\n- **Downstream tasks**: Benefits large‑scale classification, self‑supervised learning, and multi‑object recognition, confirming the generic applicability of the object‑aware bias.\n\nOverall, OAMixer provides a universal framework for injecting object‑aware inductive bias into patch mixing layers, leading to stronger intra‑object feature communication and reduced spurious object‑background correlations without requiring additional human annotations.\n"
        },
        {
            "paper_id": "PatchRot_ A Self-Supervised Technique for Training Vision Transformers",
            "summary": "## PatchRot: A Self‑Supervised Technique for Training Vision Transformers (Chhabra et al., 2022)\n\nVision Transformers (ViTs) are highly data‑hungry; labeling large datasets is costly.  PatchRot addresses this by defining a **pre‑text task that exploits both global and local cues** of an image, tailored to the token‑wise processing of ViTs.\n\n### Method\n- **Pre‑text task**: Either the whole image *or* each individual patch is rotated by one of four multiples of 90° (0°, 90°, 180°, 270°).  Crucially, **image rotation is *not* applied when rotating patches**, because combining both was found to degrade downstream performance.\n- **Prediction heads**:\n  - The **class token** (the classification head) predicts the rotation angle of the *entire* image.\n  - A set of **auxiliary MLP heads** attached to each patch token predicts the rotation angle of its corresponding patch.\n- **Loss**: Cross‑entropy terms for image‑rotation and for all patch‑rotations are summed (Eq. 1).\n- **Post‑training**: After the self‑supervised stage, the auxiliary patch MLP heads are **removed**; only the original classification head (and its encoder output) is retained for downstream tasks.  The \"Reuse MLP head\" variant is an ablation, not the default pipeline.\n\n### Training Setup\n- **Architecture**: ViT‑Lite (Hassani et al., 2021) – 6 encoder blocks, 256‑dimensional embeddings, 512‑dimensional feed‑forward expansion, 4 attention heads, dropout = 0.1.\n- **Patch size**: P = 4 for CIFAR‑10/100 and FashionMNIST, P = 8 for Tiny‑ImageNet.\n- **Buffer gap**: B = 14 % of the patch size, creating a random gap (0–2B px) between patches; input resolution is reduced (e.g., 24×24 for 32×32 images).\n- **Optimizer & schedule**: Adam with learning rate = 5 × 10⁻⁴, weight decay = 3 × 10⁻²; LR warmed up for 10 epochs then decayed cosine‑wise.\n- **Batch size**: 128 (effective batch size 128 × 5 due to the five rotated versions per image).\n- **Epochs**: 300 epochs for self‑supervised PatchRot pre‑training, followed by 200 epochs of supervised fine‑tuning on the downstream classification tasks.\n\n### Experimental Findings\n- PatchRot **outperforms** both a fully supervised baseline and the original RotNet on CIFAR‑10, CIFAR‑100, FashionMNIST, and Tiny‑ImageNet, especially when deeper encoder layers are fine‑tuned.\n- **Ablation studies** confirm that joint supervision of image‑ and patch‑rotations is essential; the \"Reuse MLP head\" variant yields slightly lower performance than the default pipeline that discards the patch heads after pre‑training.\n- Visualizations of attention maps show that PatchRot‑trained ViTs attend to broader object regions, indicating richer feature learning.\n\n### Limitations & Future Directions\n- Evaluation is limited to small‑scale datasets and the lightweight ViT‑Lite; scalability to larger ViTs (ViT‑Base/‑Large) and high‑resolution ImageNet‑level tasks remains untested.\n- The pre‑text task uses only discrete 90° rotations, potentially biasing representations toward orientation‑specific features.\n- The buffer‑gap cropping reduces effective resolution, which might hinder dense prediction tasks (e.g., segmentation).\n- Added MLP heads increase memory usage during pre‑training; comparative computational costs against methods such as DINO or MAE are not reported.\n- Robustness to distribution shift or adversarial perturbations is not explored.\n\n**Future work** should examine PatchRot’s applicability to larger ViTs and datasets, incorporate more varied geometric transformations, evaluate on detection/segmentation downstream tasks, and provide thorough analyses of computational efficiency and robustness.\n"
        },
        {
            "paper_id": "Vicinity Vision Transformer",
            "summary": "The Vicinity Vision Transformer (VVT) paper addresses the prohibitive quadratic complexity of conventional softmax attention in vision transformers, which limits their applicability to high‑resolution images. The authors identify that existing linear‑attention schemes, originally devised for natural language processing, neglect a critical inductive bias in visual data: two‑dimensional (2D) locality. To remedy this, they propose Vicinity Attention, a linear‑attention mechanism that integrates 2D Manhattan‑distance based re‑weighting, ensuring that spatially neighboring patches receive stronger attention while preserving global context. A novel Vicinity Attention Block (VAB) is introduced to alleviate the quadratic dependence on feature dimension inherent to linear attention. VAB comprises a Feature Reduction Attention (FRA) module that halves the feature dimensionality before attention computation, and a Feature Preserving Connection (FPC) that restores the original distribution via a lightweight skip‑connection resembling squeeze‑excitation. By embedding VAB within a four‑stage pyramid backbone, the resulting VVT processes full‑resolution feature maps with linear complexity in token count, enabling efficient handling of large inputs.\n\nEmpirical evaluation spans image classification on CIFAR‑100, ImageNet‑1k, and ADE20K semantic segmentation. VVT variants (Tiny, Small, Medium, Large) consistently outperform state‑of‑the‑art transformers (ViT, Swin, PVT, Twins) and competitive CNNs while using roughly 50 % fewer parameters. Notably, VVT‑L achieves 84.1 % top‑1 accuracy on ImageNet‑1k with 61.8 M parameters and exhibits a slower GFLOPs growth rate as input resolution increases. Ablation studies confirm that both the 2D locality bias and the FRA/FPC design contribute to accuracy gains; replacing Vicinity Attention with 1‑D cosine re‑weighting (cosFormer) leads to divergence on large‑scale data, underscoring the necessity of the proposed 2D formulation. Qualitative Grad‑CAM visualizations further demonstrate that VVT concentrates attention on semantically relevant regions, a direct benefit of the locality‑aware weighting.\n\nDespite these strengths, the work exhibits several limitations. First, while FRA reduces computational cost, it introduces an additional hyper‑parameter (reduction ratio) whose optimal setting is empirically determined; inappropriate choices can degrade performance, suggesting sensitivity to architecture tuning. Second, the memory analysis reveals that VVT’s absolute memory footprint remains higher than some recent efficient transformers (e.g., Performer, Linformer) for very high resolutions, indicating that linear complexity alone does not guarantee scalability on memory‑constrained hardware. Third, the experiments focus on standard benchmarks; robustness to distribution shifts, adversarial perturbations, or domain adaptation scenarios is not explored, leaving open questions about the generality of the learned locality bias. Future work could address these issues by extending Vicinity Attention to adaptive, data‑driven distance metrics, integrating dynamic reduction strategies, and evaluating the approach on broader vision modalities and robustness benchmarks.\n"
        },
        {
            "paper_id": "The Evolution of First Person Vision Methods_ A Survey",
            "summary": "# Summary of *The Evolution of First Person Vision Methods: A Survey*\n\n**Authors:** A. Betancourt, P. Morerio, C.S. Regazzoni, M. Rauterberg  \n**Affiliations:** University of Genova (Italy) and Eindhoven University of Technology (Netherlands)\n\n---\n\n## Overview\nThe paper surveys research on **First‑Person Vision (FPV)** – also called *egocentric* or *ego‑vision* – covering the period **1997‑2014**. It documents how the emergence of wearable devices such as **action cameras** and **smart‑glasses** has driven a growing interest in analysing video recorded from the wearer’s perspective.\n\n## Publication Trend\n- Figure 1 in the original article shows a **steep increase** in the number of FPV‑related publications from 1997 to 2014, with a notable rise after the introduction of commercial smart‑glasses prototypes.\n- The authors collected more than one hundred papers to map this evolution.\n\n## Commercial Patents (2012)\n- The survey highlights two prominent patents filed in **2012**:\n  1. **Google Glass** – U.S. Patent D659,741 (May 15 2012).\n  2. **Microsoft Augmented‑Reality Glasses** – U.S. Patent Application 20120293548 (Nov 22 2012).\n- These patents illustrate the transition from research prototypes to commercial interest.\n\n## Application Domains Mentioned\nThe authors list a wide range of potential application areas for FPV technology, including:\n- **Military and security**\n- **Enterprise and industrial**\n- **Tourism services**\n- **Mass surveillance**\n- **Medicine and healthcare**\n- **Driving assistance**\n\n## Technical Landscape (1997‑2014)\n- Early FPV analysis relied on **hand‑crafted image features** (e.g., colour, optical flow, spatio‑temporal interest points) combined with conventional classifiers.\n- Methods were often designed to address specific tasks such as **object detection**, **activity recognition**, and **human‑machine interaction**.\n- Real‑time processing was emphasized because wearable platforms have limited computational resources and battery capacity.\n- The survey notes the importance of **multi‑modal sensor suites** (e.g., cameras, inertial sensors) embedded in smart‑glasses, as summarized in Table 2 of the paper.\n\n## Practical Challenges\nThe authors identify several challenges that arise when working with FPV video:\n- **Severe camera motion** and rapid changes in illumination, which complicate feature extraction and tracking.\n- **Privacy concerns** due to continuous recording from a first‑person viewpoint.\n- **Battery life** constraints of wearable devices, limiting the feasibility of computationally intensive algorithms.\n- **Algorithmic difficulty** in achieving robust performance under the above conditions.\n\n## Datasets\nA brief overview of publicly available FPV datasets is provided (Section 3 of the paper), serving as benchmarks for evaluating the described methods.\n\n## Funding Acknowledgement\nThe work was partially supported by the **Erasmus Mundus Joint Doctorate** program in *Interactive and Cognitive Environments*, funded by the European Commission’s EACEA.\n\n## Outlook\nThe survey concludes that, while substantial progress has been made, future research must address:\n- Development of **energy‑efficient algorithms** suitable for wearable hardware.\n- **Robust multi‑sensor fusion** to mitigate motion and lighting issues.\n- Creation of **standardized evaluation protocols** that consider privacy and user‑acceptance aspects.\n\nOverall, the paper provides a historical map of FPV research up to 2014, highlighting the rapid growth of the field, its commercial momentum, diverse application potential, and the technical hurdles that must be overcome for broader adoption.\n"
        }
    ],
    "final_summary_my_pipe": "Final Summary: # Integrated Summary of Recent Vision Transformer Advances and First‑Person Vision Survey\n\n## 1. Surface Vision Transformer (SiT)\n- **Motivation & Pipeline**: Dahan *et al.* (2022) cast cortical surface analysis as a sequence‑to‑sequence problem. Genus‑zero cortical meshes are projected onto a regularly tessellated icosphere, partitioned into equal‑area triangular patches, each flattened into a token for a standard ViT encoder. A learnable target token and sinusoidal positional embeddings enable operation on both template‑aligned and native (unregistered) surfaces without explicit rotational equivariance.\n- **Model variants**: DeiT‑Tiny (5.5 M parameters) → **SiT‑tiny**; DeiT‑Small (21.6 M) → **SiT‑small**.\n- **Training strategies**:\n  1. From scratch.\n  2. ImageNet‑pretrained weights via *timm*.\n  3. Self‑supervised **masked‑patch‑prediction (MPP)** where 50 % of patches are corrupted and reconstructed (MSE loss on masked patches only).\n- **Class‑imbalance handling**: Because term neonates outnumber preterm ones, **balanced‑batch sampling** is performed during training to mitigate bias.\n- **Augmentation effects**: Adding **rotation and dropout augmentations** significantly improves **gestational‑age (GA) prediction**, but has negligible impact on **post‑menstrual age (PMA)** prediction.\n- **Results (dHCP neonatal cortical surfaces)**:\n  | Model | Pre‑training | PMA MAE (weeks) – template / native | GA MAE (weeks) – template / native |\n  |---|---|---|---|\n  | SiT‑small | MPP | **0.55 / 0.63** (avg ≈ 0.59) | **1.02 / 1.21** (avg ≈ 1.12) |\n  | SiT‑small | ImageNet | 0.59 / 0.71 | 1.13 / 1.30 |\n  | SiT‑tiny | MPP | 0.58 / 0.64 | 1.03 / 1.31 |\n  | SiT‑tiny | ImageNet | 0.67 / 0.70 | 1.11 / 1.20 |\n  The MoNet baseline reaches **MAE = 0.57 / 0.61 weeks for PMA**, not for GA. SiT matches or surpasses the best surface CNNs (S2CNN, GConvNet, ChebNet) and degrades less on unregistered data, indicating a degree of transformation invariance.\n- **Limitations & Future Work**: Restricted to genus‑zero manifolds, fixed patch size may lose fine detail, vanilla ViT encoder could be replaced by hierarchical or manifold‑aware attention, and pre‑training could be broadened beyond ImageNet and MPP.\n\n---\n\n## 2. Glance‑and‑Gaze Vision Transformer (GG‑Transformer)\n- **Problem**: Vanilla self‑attention scales quadratically with token length, limiting high‑resolution dense tasks.\n- **Architecture**:\n  - **Glance branch**: Input is split into *adaptively‑dilated partitions*; each partition samples tokens globally, yielding linear‑complexity global attention.\n  - **Gaze branch**: After merging partitions, a lightweight **depth‑wise convolution** (3×3 or adaptive kernel) injects local context.\n  - The two streams are merged, preserving spatial resolution.\n- **Integration**: The **GG‑MSA** block replaces standard MSA in Swin‑style and DeiT back‑bones.\n- **Quantitative gains**:\n  - **ImageNet‑1K (224×224)**: GG‑T achieves **82.0 %** top‑1 (+0.8 % over Swin‑T); GG‑S reaches **83.4 %** (+0.2 % over Swin‑S).\n  - **ADE20K semantic segmentation (UperNet)**: GG‑T attains **46.4 %** mIoU, surpassing ResNet‑50, PVT‑Small, and Swin‑T (multi‑scale) by **3.6 %**, **1.6 %**, and **0.6 %**, respectively. GG‑S obtains **48.4 %** mIoU, comparable to Swin‑S.\n  - **COCO detection & instance segmentation (Mask R‑CNN / Cascade Mask R‑CNN)**:\n    * GG‑T: box AP **44.1** (↑0.4) and mask AP **39.9** (↑0.1) over Swin‑T.\n    * GG‑S: box AP **45.7** (↑0.3) and mask AP **41.3** (↑0.1) over Swin‑S.\n- **Ablations**: Removing either Glance or Gaze drops accuracy by 3–4 %; depth‑wise convolution outperforms a self‑attention‑based Gaze while staying within the same FLOP budget.\n- **Limitations**: Data‑efficiency challenges remain, performance may suffer on extreme aspect‑ratio images, and fixed‑size positional encodings can degrade when testing at resolutions far from training.\n\n---\n\n## 3. ViTAEv2 – Vision Transformer Advanced by Exploring Inductive Bias\n- **Core Idea**: Explicitly inject **locality** and **scale‑invariance** into ViTs via two elementary cells that can be stacked isotropically or in a multi‑stage fashion.\n- **Cells**:\n  - **Reduction Cell (RC)** – Pyramid Reduction Module with atrous convolutions (dilation sets S₁=[1,2,3,4], S₂=[1,2,3], S₃=[1,2]) and down‑sampling ratios 4×, 2×, 2×.\n  - **Normal Cell (NC)** – Parallel Convolutional Module (three 3×3 convs + BN + SiLU) followed by MHSA and FFN.\n- **Fusion strategy**: **Early fusion** of PCM and MHSA **before** the FFN, combined with **BatchNorm**, yields the best ImageNet‑1K top‑1 (~69.9 %). Late fusion performs worse.\n- **Attention types per stage** (example ViTAEv2‑S): `W, W, F, F` (window attention in first two stages, full attention in last two), reducing memory while keeping accuracy.\n- **Training regimes**:\n  - Standard supervised training on **ImageNet‑1K**.\n  - **MAE self‑supervised pre‑training** (75 % mask) followed by fine‑tuning for the larger variants (ViTAE‑B, ViTAE‑L, ViTAE‑H).\n- **Model sizes**:\n  - **ViTAE‑H** (isotropic) – 644 M parameters.\n  - **ViTAE‑B** (MAE‑pre‑trained) – 89.7 M parameters.\n  - **ViTAE‑L** – 311 M parameters.\n  - **ViTAEv2‑S** (multi‑stage) – 19.2 M parameters; trained on only **5 %** of ImageNet‑1K and still reaches **82.6 %** top‑1.\n- **Downstream performance**:\n  - **COCO detection (Mask RCNN)** – APᵇ 46.3 % (ViTAEv2‑S) vs. ResNet‑50 42.0 %.\n  - **ADE20K segmentation** – 45.0 % mIoU (single‑scale) vs. ResNet‑50 42.1 %.\n  - **Animal pose (AP‑10K)** – AP 0.718, outperforming ResNet‑50 and Swin‑T.\n- **Limitations**: Added architectural complexity, still data‑hungry for the biggest models, focus solely on image‑centric tasks, and limited robustness evaluation.\n\n---\n\n## 4. Evo‑ViT – Slow‑Fast Token Evolution\n- **Motivation**: Reduce ViT inference cost without destroying spatial structure; avoid costly pre‑training required by many token‑pruning methods.\n- **Mechanism**:\n  1. **Structure‑preserving token selection** using **class‑token attention** (CLS‑attention) that is further refined by an **evolved global class attention** across layers (Eq. 4).\n  2. Top‑k tokens become **informative (object) tokens** processed by the full ViT pipeline (slow path).\n  3. Remaining tokens are merged into a **single placeholder token**, updated by the same MSA + FFN (fast path); the resulting residual is broadcast back to every placeholder token (Eq. 7).\n- **Training schedule (layer‑to‑stage)**:\n  - Token selection begins at the **5th encoder layer** (DeiT) or **9th layer** (LeViT) and is applied for the first **200 epochs**.\n  - In the final **100 epochs**, selection is retained **only at the beginning of each stage**, while updating continues in every layer.\n- **Auxiliary loss**: An additional **CLS‑token loss** is added to the standard classification loss to stabilise the evolving class‑attention signal.\n- **Results**: On ImageNet‑1K, Evo‑ViT speeds up DeiT‑S by **>60 % throughput** (2 536 → 4 027 images/s) with a mere **0.4 %** top‑1 drop (72.0 % → 71.6 %). Comparable or better accuracy vs. token‑pruning baselines, with larger speed gains.\n- **Limitations**: Evaluation limited to classification; several hyper‑parameters (keeping ratios, evolution weight α) are set empirically.\n\n---\n\n## 5. Quadrangle Attention (QA)\n- **Concept**: Replace fixed rectangular windows with **learnable quadrangles** that better align with object shapes.\n- **Quadrangle regression head**: Very shallow – average‑pool → LeakyReLU → 1×1 convolution – predicts a projective transformation matrix for each default square window.\n- **Operations**: Tokens inside the transformed quadrangle are sampled via **bilinear interpolation** and fed to standard Q‑K‑V attention.\n- **Regularisation**: A term **λ** (set to **1** in experiments) penalises quadrangles that fall largely outside the feature map, stabilising training. Extreme λ values cause divergence.\n- **Out‑of‑bounds handling**: Tokens sampled outside the feature map are **zero‑padded**.\n- **Computational cost**: Bilinear sampling adds **<5 %** of total FLOPs; inference slows by **≈13 %** on an A100 due to unoptimised kernels.\n- **Performance** (QFormer family):\n  - **ImageNet‑1K**: QFormerh‑B 84.1 % top‑1 (+0.7 % over Swin‑B); QFormerh‑T 82.5 % (+0.9 %).\n  - **COCO (Mask‑RCNN)**: QFormerh‑T box AP 45.9 (+0.7), mask AP 41.5 (+0.7).\n  - **COCO (Cascade‑RCNN, 3×)**: box AP 49.8 (+0.7), mask AP 43.0 (+0.7).\n  - **ADE20K (UPerNet)**: mIoU 46.9 (+2.4) for QFormerh‑T.\n  - **COCO keypoints**: AP 77.2 (+0.4).\n- **Ablation**: Each elementary transformation (scale‑shift, shear, rotation, projection) contributes positively; the full composition yields the best gains.\n- **Limitations**: Shallow regression head may restrict modeling of extreme perspective deformations; out‑of‑bounds padding can lead to sparse gradients; compatibility with other self‑supervised or multimodal pre‑training remains untested; runtime overhead could be reduced with fused CUDA kernels.\n\n---\n\n## 6. OAMixer – Object‑aware Mixing Layer\n- **Goal**: Inject a sample‑specific **object‑level inductive bias** into any patch‑mixing mechanism (self‑attention, MLP, convolutional mixing).\n- **Mask formulation**: For layer *l*, the similarity mask is\n  \\[ M_{ij}=\\exp\\bigl(-\\kappa^{(l)}\\cdot d(y_i, y_j)\\bigr) \\]\n  where \\(y_i\\) and \\(y_j\\) are object‑level labels (obtained unsupervised or weakly‑supervised), \\(d\\) is a distance function, and \\(\\kappa^{(l)}\\) is a learnable scalar controlling bias strength.\n- **Application**: The mask is **applied element‑wise before** the linear operators of:\n  1. **Self‑attention** – scales the attention matrix prior to softmax, yielding an object‑aware attention \\(\\tilde A\\).\n  2. **Feed‑forward MLP** – modulates the linear projection of token features.\n  3. **Convolutional mixing** – multiplies the linearised Toeplitz kernel (equivalent to a 2‑D convolution) before convolution.\n- **Results**:\n  - **ImageNet‑1K classification**: DeiT‑B top‑1 rises from **78.45 %** to **82.18 %** (+3.73 %).\n  - **Self‑supervised (DINO) linear probe**: DeiT‑T improves from **59.37 %** to **61.16 %** (+1.79 %).\n  - Demonstrated better robustness to background shifts compared with methods that only use spatial bias (ConViT) or token‑level supervision (TokenLabeling).\n- **General applicability**: Works with ViTs, MLP‑Mixers, ConvMixers, requiring only the mask computation per layer.\n\n---\n\n## 7. PatchRot – Self‑Supervised Pre‑text for ViTs\n- **Pre‑text task**: Either the **whole image** or each **individual patch** is rotated by one of four multiples of 90° (0°, 90°, 180°, 270°). Image rotation is *not* applied when rotating patches (the combination harms downstream performance).\n- **Prediction heads**:\n  - **Class token** predicts the image‑rotation angle.\n  - **Auxiliary MLP heads** attached to each patch token predict the patch‑rotation angle.\n- **Loss**: Sum of cross‑entropy losses for image‑rotation and all patch‑rotations.\n- **Training details**:\n  - **Architecture**: ViT‑Lite (6 encoder blocks, 256‑dim embeddings, 512‑dim FFN, 4 heads, dropout = 0.1).\n  - **Patch size**: P = 4 for CIFAR‑10/100 & FashionMNIST; P = 8 for Tiny‑ImageNet.\n  - **Buffer‑gap cropping**: A random gap of size **B = 14 % of the patch size** (0–2B px) is inserted between patches, reducing effective resolution (e.g., 24×24 for 32×32 images).\n  - **Optimizer & schedule**: Adam, learning rate **5 × 10⁻⁴**, weight decay **3 × 10⁻²**, warm‑up for 10 epochs, then cosine decay.\n  - **Epochs**: **300** epochs for self‑supervised pre‑training, followed by **200** epochs of supervised fine‑tuning on the downstream classification task.\n- **Results**: Outperforms both a fully supervised baseline and RotNet on CIFAR‑10, CIFAR‑100, FashionMNIST, and Tiny‑ImageNet, especially when deeper encoder layers are fine‑tuned.\n- **Limitations**: Tested only on small‑scale datasets and ViT‑Lite; scalability to larger ViTs and ImageNet‑scale tasks remains unverified; only discrete 90° rotations are used, potentially biasing representations; additional MLP heads increase memory during pre‑training.\n\n---\n\n## 8. Vicinity Vision Transformer (VVT)\n- **Key innovation**: **Vicinity Attention**, a linear‑attention mechanism that re‑weights token interactions by **2‑D Manhattan distance**, enforcing spatial locality while retaining global context.\n- **Vicinity Attention Block (VAB)**:\n  - **Feature Reduction Attention (FRA)** halves the feature dimension before the linear‑complexity attention computation.\n  - **Feature Preserving Connection (FPC)** restores the original dimensionality via a lightweight skip‑connection (squeeze‑excitation‑like).\n- **Architecture**: Four‑stage pyramid backbone; linear complexity in token count enables full‑resolution processing.\n- **Performance** (ImageNet‑1K):\n  - **VVT‑L** – **84.1 %** top‑1 with **61.8 M** parameters, achieving comparable or better accuracy than ViT, Swin, PVT while using ~50 % fewer parameters.\n  - FLOPs grow slower with input resolution, making VVT attractive for high‑resolution inputs.\n- **Ablations**: Both the 2‑D locality bias and the FRA/FPC design contribute; replacing Vicinity Attention with 1‑D cosine re‑weighting (cosFormer) causes divergence on large‑scale data.\n- **Limitations**:\n  - **Memory footprint** is still higher than some recent efficient transformers (e.g., Performer, Linformer) for very high resolutions, showing that linear token complexity alone does not guarantee memory scalability.\n  - The **reduction‑ratio hyper‑parameter** is sensitive; improper settings degrade performance, indicating a need for careful tuning.\n\n---\n\n## 9. Survey of First‑Person Vision (FPV) Methods (1997‑2014)\n- **Scope**: Comprehensive review of egocentric/first‑person vision research from 1997 to 2014, covering over **100** papers.\n- **Trend**: Publication count rises sharply after the introduction of commercial smart‑glasses; Figure 1 in the original paper visualises this growth.\n- **Technical landscape**: Early approaches rely on handcrafted features (color, optical flow, spatio‑temporal interest points) combined with conventional classifiers for tasks such as object detection, activity recognition, and human‑machine interaction.\n- **Real‑time emphasis**: Because wearable devices have limited compute and battery, **real‑time processing** is a recurring design constraint.\n- **Challenges**: Severe camera motion, rapid illumination changes, privacy concerns, and battery life limit algorithmic choices.\n- **Patents (2012)**: Google Glass (U.S. Patent D659,741, May 15 2012) and Microsoft Augmented‑Reality Glasses (U.S. Patent Application 20120293548, Nov 22 2012) illustrate the shift toward commercial interest.\n- **Application domains**: Military & security, enterprise & industrial, tourism services, mass surveillance, medicine & healthcare, driving assistance.\n- **Outlook**: Calls for energy‑efficient algorithms, robust multimodal sensor fusion, and standardized, privacy‑aware evaluation protocols.\n\n---\n\n## 10. Consolidated Takeaways\n- Vision Transformers are rapidly evolving to address **computational efficiency**, **inductive bias incorporation**, and **domain‑specific constraints** (e.g., cortical surfaces, egocentric video, high‑resolution imagery).\n- Across the board, **augmentation strategies**, **self‑supervised pre‑training**, and **architectural hybrids** (convolution + attention, linear‑attention with locality) consistently improve data efficiency and robustness.\n- Limitations that remain common include **sensitivity to hyper‑parameters**, **memory scalability at extreme resolutions**, and **restricted evaluation to image‑centric tasks**.\n- Future research directions highlighted repeatedly are **hierarchical or adaptive attention mechanisms**, **broader self‑supervision**, **cross‑modal fusion**, and **privacy‑preserving, real‑time algorithms** for wearable and biomedical applications.\n"
}